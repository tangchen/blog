[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, welcome to my personal website. I am currently a PhD candidate at the School of Labor and Employment Relations, University of Illinois at Urbana-Champaign. My advisor is Dr. Daniel A. Newman. I am also an affiliated doctoral researcher at Purdue Recruitment and Selection Research Lab led by Dr. Q. Chelsea Song.\nMy research interests are People Analytics and other topics related to Human Resources and Organizational Behavior. For more information about me, please see my CV [here](https://tangc.net/cv.html)."
  },
  {
    "objectID": "about.html#please-note",
    "href": "about.html#please-note",
    "title": "About",
    "section": "Please note…",
    "text": "Please note…\nThis website is actually my personal notebook. It contains stuff that I’m working on, or some pieces of information that I find interesting or important. They are very unstructured and probably contain a lot of errors/typos/thoughtos. ;o)"
  },
  {
    "objectID": "posts/2019-09-01-bias-variance-breakdown/index.html",
    "href": "posts/2019-09-01-bias-variance-breakdown/index.html",
    "title": "A Very Detailed Bias-Variance Breakdown",
    "section": "",
    "text": "Although the concept of bias-variance trade-off is often discussed in machine learning textbooks, e.g., Bishop (2006), Hastie, Tibshirani, and Friedman (2009), James et al. (2013), I also find it important in almost any occasions in which we need to fit a statistical model on a data set with a limited number of observations.\nTo better understand the trade-off, we should be clear about the bias-variance decomposition. I prefer to call it bias-variance breakdown cause there are fewer syllables. This post is an attempt to go through the breakdown in a very detailed manner, mainly for my future reference. It may not be 100% correct because I’m very new to this topic. I will make changes if there is anything wrong."
  },
  {
    "objectID": "posts/2019-09-01-bias-variance-breakdown/index.html#what-is-being-broken-down",
    "href": "posts/2019-09-01-bias-variance-breakdown/index.html#what-is-being-broken-down",
    "title": "A Very Detailed Bias-Variance Breakdown",
    "section": "What is being broken down?",
    "text": "What is being broken down?\nFirst off, the bias-variance concept lives in theory. In practice, we have no way to separate bias and variance. What we could observe is just the sum of them, as well as something called “irreducible error”. We will cover that later in this post.\nSince we are talking about theory, let’s make up the thing that needs to be broken down. Let’s assume that we are interested in studying the relationship between \\(X\\) and \\(Y\\). Suppose the relationship is: \\[Y = f(X) + \\epsilon\\],\nwhere \\(\\epsilon\\) is i.i.d. \\(E(\\epsilon) = 0\\) and \\(\\text{Var}(\\epsilon) = \\sigma^2\\).\nThen we obtain a data set, \\(D\\), that has \\(x_i\\) and \\(y_i\\) (\\(D = [x_i, y_i]\\)). Note that in the data set, \\(X\\) and \\(Y\\) are lowercase and have subscripts. This is because we are referring to observed data of \\(X\\) and \\(Y\\).\nAs usual, we fit a model to \\(D\\) and obtain the model \\(\\hat{f}(x_i)\\). There is a “hat” on \\(f(x_i)\\) because we are estimating the true model, \\(f()\\). After getting \\(\\hat{f}()\\), we are interested in how this model would perform in the future. So we have to come up with a way to measure the performance of the model when applied to new data. The most common way to measure the predictive performance of a model is mean squared error (MSE) on a new data set \\([x^\\ast, y^\\ast],\\) or theoretically, the expected squared error:\n\\[E[(y^\\ast - \\hat{f}(x^\\ast))^2]\\]\nThis is the thing to be broken down."
  },
  {
    "objectID": "posts/2019-09-01-bias-variance-breakdown/index.html#breaking-it-down",
    "href": "posts/2019-09-01-bias-variance-breakdown/index.html#breaking-it-down",
    "title": "A Very Detailed Bias-Variance Breakdown",
    "section": "Breaking it down",
    "text": "Breaking it down\nBefore we go further, we need to make sure we are clear about which is which. Since this is an expected value, there must be random variables in this equation. What is random here? First, let’s take a look at \\(\\hat{f}(x^\\ast)\\).\nWe know that \\(\\hat{f}()\\) comes from \\(D\\), and \\(D\\) contains \\(\\epsilon\\), because the true model we assumed is \\(Y = f(X) + \\epsilon\\). So \\(\\hat{f}()\\) also contains \\(\\epsilon\\) and hence it is a random variable. Second, what about \\(y^\\ast\\)? Since \\([x^\\ast, y^\\ast]\\) is a sample from the true model, it again contains \\(\\epsilon\\), therefore \\(y^\\ast\\) is also a random variable.\nLet’s now play a mathematical trick:\n\\[E[(y^\\ast - \\hat{f}(x^\\ast))^2] = E[(y^\\ast - f(x^\\ast) + f(x^\\ast) - \\hat{f}(x^\\ast))^2]\\]\nHere we just add and subtract \\(f(x^\\ast)\\), nothing is changed. Let \\(A = y^\\ast - f(x^\\ast)\\) and \\(B = f(x^\\ast) - \\hat{f}(x^\\ast)\\). Then the above equation becomes:\n\\[\\begin{aligned}\nE[(A + B)^2] &= E[A^2 + B^2 + 2AB] \\\\\n&= E[A^2] + E[B^2] + 2E[AB]\n\\end{aligned}\\]\nLet’s put \\(A\\) and \\(B\\) back\n\\[E[(y^\\ast - f(x^\\ast))^2] + E[(f(x^\\ast) - \\hat{f}(x^\\ast))^2] + 2E\\{[y^\\ast - f(x^\\ast)][f(x^\\ast) - \\hat{f}(x^\\ast)]\\}\\]\nThis is very complicated, especially the long thing on the right. Let’s first expand it: \\[2\\{E[y^\\ast f(x^\\ast)] - E[y^\\ast \\hat{f}(x^\\ast)] - E[f(x^\\ast) f(x^\\ast)] + E[f(x^\\ast) \\hat{f}(x^\\ast)]\\}\\]\nAccording to our theoretical model, we know that \\(y^\\ast = f(x^\\ast) + \\epsilon\\), so\n\\[2\\{E[(f(x^\\ast) + \\epsilon) f(x^\\ast)] - E[(f(x^\\ast) + \\epsilon) \\hat{f}(x^\\ast)] - E[f(x^\\ast) f(x^\\ast)] + E[f(x^\\ast) \\hat{f}(x^\\ast)]\\}\\]\n\\[2\\{[f(x^\\ast)]^2 - E[f(x^\\ast) \\hat{f}(x^\\ast) + \\epsilon \\hat{f}(x^\\ast)] - [f(x^\\ast)]^2 + E[f(x^\\ast) \\hat{f}(x^\\ast)]\\}\\]\n\\[2\\{[f(x^\\ast)]^2 - E[f(x^\\ast) \\hat{f}(x^\\ast)] + E[\\epsilon \\hat{f}(x^\\ast)] - [f(x^\\ast)]^2 + E[f(x^\\ast) \\hat{f}(x^\\ast)]\\}\\]\nFour terms cancel out, the term \\(E[\\epsilon \\hat{f}(x^\\ast)] = 0\\), because \\(\\epsilon\\) and \\(\\hat{f}(x^\\ast)\\) are independent. Therefore \\(E[\\epsilon \\hat{f}(x^\\ast)] = E[\\epsilon] \\times E[\\hat{f}(x^\\ast)] = 0\\).\nOK. That long thing becomes zero and we are left with\n\\[E[(y^\\ast - f(x^\\ast))^2] + E[(f(x^\\ast) - \\hat{f}(x^\\ast))^2]\\]\nNow let’s play a similar trick on the second term, the first term remains unchanged.\n\\[E[(y^\\ast - f(x^\\ast))^2] + E\\{[f(x^\\ast) - E[\\hat{f}(x^\\ast)] + E[\\hat{f}(x^\\ast)] - \\hat{f}(x^\\ast)]^2\\}\\]\nAgain let \\(A = f(x^\\ast) - E[\\hat{f}(x^\\ast)]\\) and \\(B = E[\\hat{f}(x^\\ast)] - \\hat{f}(x^\\ast)\\).\n\\[E[(y^\\ast - f(x^\\ast))^2] + E[(A + B)^2]\\]\n\\[E[(y^\\ast - f(x^\\ast))^2] + E[A^2] + E[B^2] + 2E[AB]\\]\nPlug in \\(A\\) and \\(B\\).\n\\[E[(y^\\ast - f(x^\\ast))^2] + E\\{[f(x^\\ast) - E[\\hat{f}(x^\\ast)]]^2\\} + E\\{[E[\\hat{f}(x^\\ast)] - \\hat{f}(x^\\ast)]^2\\}\\] \\[ + 2E\\{[f(x^\\ast) - E[\\hat{f}(x^\\ast)]][E[\\hat{f}(x^\\ast)] - \\hat{f}(x^\\ast)]\\}\\]\nLet’s again look at the most annoying thing on the second row. We notice that (1) \\(f(x^\\ast) - E[\\hat{f}(x^\\ast)]\\) is a constant, and (2) the expected value of \\(E[\\hat{f}(x^\\ast)] - \\hat{f}(x^\\ast)]\\) is just \\(E\\{E[\\hat{f}(x^\\ast)]\\} - E[\\hat{f}(x^\\ast)]\\). This equals \\(E[\\hat{f}(x^\\ast)] - E[\\hat{f}(x^\\ast)] = 0\\).\nSo we are left with\n\\[E[(y^\\ast - f(x^\\ast))^2] + E\\{[f(x^\\ast) - E[\\hat{f}(x^\\ast)]]^2\\} + E\\{[E[\\hat{f}(x^\\ast)] - \\hat{f}(x^\\ast)]^2\\}\\]\nWe also notice that, in the second term, both \\(f(x^\\ast)\\) and \\(E[\\hat{f}(x^\\ast)]\\) are constants, so we can drop the expectation operator.\n\\[E[(y^\\ast - f(x^\\ast))^2] + [f(x^\\ast) - E[\\hat{f}(x^\\ast)]]^2 + E\\{[E[\\hat{f}(x^\\ast)] - \\hat{f}(x^\\ast)]^2\\}\\]\nCool! We have finished the breakdown."
  },
  {
    "objectID": "posts/2019-09-01-bias-variance-breakdown/index.html#naming-things",
    "href": "posts/2019-09-01-bias-variance-breakdown/index.html#naming-things",
    "title": "A Very Detailed Bias-Variance Breakdown",
    "section": "Naming things",
    "text": "Naming things\nAs a final step, let’s name the three terms in\n\\[E[(y^\\ast - f(x^\\ast))^2] + [f(x^\\ast) - E[\\hat{f}(x^\\ast)]]^2 + E\\{[E[\\hat{f}(x^\\ast)] - \\hat{f}(x^\\ast)]^2\\}\\]\nThe first term, \\(E[(y^\\ast - f(x^\\ast))^2]\\), is the variance of \\(\\epsilon\\), which is \\(\\sigma^2\\). We call this “irreducible error” or “irreducible noise”.\nThe second term, \\([f(x^\\ast) - E[\\hat{f}(x^\\ast)]]^2\\), is the “squared bias”, because the definition of the bias of an estimator is \\(\\text{bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\\).\nThe third term, \\(E\\{[E[\\hat{f}(x^\\ast)] - \\hat{f}(x^\\ast)]^2\\}\\)is, by definition, the variance of \\(\\hat{f}(x^\\ast)\\).\nSo to put everything together,\n\\[E[(y^\\ast - \\hat{f}(x^\\ast))^2] = \\text{Var}(\\epsilon) + \\text{bias}^2(\\hat{f}(x^\\ast)) + \\text{Var}(\\hat{f}(x^\\ast))\\]"
  },
  {
    "objectID": "posts/2019-09-01-bias-variance-breakdown/index.html#reference",
    "href": "posts/2019-09-01-bias-variance-breakdown/index.html#reference",
    "title": "A Very Detailed Bias-Variance Breakdown",
    "section": "Reference",
    "text": "Reference\nBishop, Christopher M. (2006). Pattern recognition and machine learning. New York: Springer,\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. (2013). An introduction to statistical learning: with applications in R. New York :Springer,\nHastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer.\n(Image credit: http://scott.fortmann-roe.com/docs/BiasVariance.html)"
  },
  {
    "objectID": "posts/2020-01-09-basic-linear-algebra/index.html",
    "href": "posts/2020-01-09-basic-linear-algebra/index.html",
    "title": "Some Basic Linear Algebra",
    "section": "",
    "text": "Purely out of curiosity, I’m recently reading the deep learning book (Goodfellow, Bengio, & Courville, 2016). I noticed that Chapter 2 Linear Algebra is a very quick and effective summary of linear algebra. It provides the right amount of linear algebra for deep learning, as well as machine learning and statistics needed for my research.\nBelow are excerpts from the deep learning book all credit goes to Dr. Ian Goodfellow and his colleagues.\n\n\n\n\nscalar: just a single number\nvector: an array of numbers\nmatrix: a 2-dimensional array of numbers\ntensor: an n-dimensional array of number\n\n\n\n\nThe transpose of a matrix is the mirror image of the matrix across the main diagonal:\n\\[A_{i,j}^{\\top} = A_{j,i}\\]\nTranspose of a scalar is itself. Transpose of a column vector is a row vector and vice versa.\n\n\n\n\n\nThe matrix product of matrices \\(A\\) and \\(B\\) is a third matrix \\(C\\). In order for this product to be defined, \\(A\\) must have the same number of columns as \\(B\\) has rows. If \\(A\\) is of shape \\(m \\times n\\) and \\(B\\) is of shape \\(n \\times p\\), then \\(C\\) is of shape \\(m \\times p\\).\n\\[C_{i, j} = \\sum_k A_{i, k} B_{k, j}\\]\n\n\n\n\\[A \\odot B\\]\n\n\n\nThe dot product between two vectors \\(x\\) and \\(y\\) of the same length is the matrix product \\(x^\\top y\\). the matrix product \\(C\\) can be thought of as the dot products of each corresponding row in \\(A\\) and column in \\(B\\).\n\n\n\n\nDistributive: \\(A(B + C)=AB + AC\\)\nAssociative: \\(A(BC) = (AB)C\\)\nNot commutative: \\(AB = BA\\) is not always true\nBut dot product between two vectors is commutative: \\(x^\\top y = y^\\top x\\)\nTranspose of a matrix product: \\((AB)^\\top = B^\\top A^\\top\\) (this can be used to prove \\(x^\\top y = y^\\top x\\))\n\n\n\n\n\nIdentity matrix: denoted as \\(I_n\\), all its entries along the main diagonal are 1, all the others are zero.\nThe inverse of \\(A\\), denoted as \\(A^{-1}\\), is defined as the matrix such that\n\\[A^{-1}A = I_n\\]\n\n\n\nA linear combination of some set of vectors \\(\\{v^{(1)}, ... , v^{(n)}\\}\\) is given by multiplying each vector \\(v^{(i)}\\) by a corresponding scalar coefficient and adding the results:\n\\[\\sum_i c_iv^{(i)}\\]\nThe span of a set of vectors is the set of all points obtainable by linear combination of the original vectors.\nDetermining whether \\(Ax = b\\) has a solution thus amounts to testing whether b is in the span of the columns of \\(A\\). This particular span is known as the column space or the range of \\(A\\).\n\n\n\nNorm is the measure of the size of a vector. The \\(L^p\\) norm is defined as:\n\\[\\|x\\|_p = \\Big(\\sum_i |x_i|^p\\Big)^{\\frac{1}{p}}\\]\nThe Euclidean norm, or \\(L^2\\) norm, is used very frequently. It is also common to use the squared \\(L^2\\) norm, which is simply \\(x^{\\top} x\\).\nHowever squared \\(L^2\\) norm is undesirable because it changes very slowly near the origin. So when it is important to discriminate between values that are exactly zero and values that are very small but nonzero, we could use \\(L^1\\) norm:\n\\[\\|x\\|_1 = \\sum_i|x_i|\\]\nMax norm is also common, which is defined by the absolute value of the element with the largest magnitude in the vector:\n\\[\\|x\\|_{\\infty} = \\max_i|x_i|\\]\nAbove norms describe the size of vectors. Frobenius norm can measure the size of a matrix:\n\\[\\|A\\|_F = \\sqrt{\\sum_{i, j} A^2_{i,j}}\\]\nThis is analogous to the \\(L^2\\) norm of a vector.\nThe dot product of two vectors can be written in terms of norms:\n\\[x^{\\top}y = \\|x\\|_2\\|y\\|_2\\cos{\\theta}\\]\nwhere \\(\\theta\\) is the angle between \\(x\\) and \\(y\\).\n\n\n\n\n\na matrix \\(D\\) is diagonal if and only if \\(D_{i, j} = 0\\) for all \\(i \\ne j\\). We write \\(\\text{diag}(v)\\) to denote a square diagonal matrix whose diagonal entries are given by the entries of the vector \\(v\\).\nDiagonal matrices are interesting because:\n\nMultiplying by a diagonal matrix is very computationally efficient: \\(\\text{diag}(v) x = v \\odot x\\)\nInverting a diagonal matrix is easy: \\(\\text{diag}(v)^{-1} = \\text{diag}(1/v_1, ..., 1/v_n)^{\\top}\\)\n\nDiagonal matrices do not need to be square. For a non-square diagonal matrix \\(D\\), the product \\(Dx\\) will involve scaling each element of \\(x\\), and either concatenating some zeros to the result if \\(D\\) is taller than it is wide, or discarding some of the last elements of the vector if \\(D\\) is wider than it is tall.\n\n\n\nA symmetric matrix is any matrix that is equal to its own transpose:\n\\[A = A^{\\top}\\]\n\n\n\nA unit vector is a vector with unit norm:\n\\[\\|x\\|_2 = 1\\]\n\n\n\nVector \\(x\\) and vector \\(y\\) are orthogonal to each other if \\(x^{\\top}y = 0\\). If the vectors are not only orthogonal but also have unit norm, we call them orthonormal.\nAn orthogonal matrix is a square matrix whose rows are mutually orthonormal and whose columns are mutually orthonormal:\n\\[A^{\\top}A = AA^{\\top} = I\\]\nOrthogonal matrices are of interest because their inverse is very cheap to compute:\n\\[A^{-1} = A^{\\top}\\]\nCounterintuitively, rows and columns of an orthogonal matrix are not merely orthogonal but fully orthonormal. There is no special term for a matrix whose rows or columns are orthogonal but not orthonormal.\nDefinition from Wikipedia: “An orthogonal matrix is a square matrix whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors).”\n(To be continued…)\n(Image credit: https://en.wikipedia.org/wiki/Matrix_%28mathematics%29)"
  },
  {
    "objectID": "posts/2021-05-31-tmag2021-data-leakage/index.html",
    "href": "posts/2021-05-31-tmag2021-data-leakage/index.html",
    "title": "Data Leakage",
    "section": "",
    "text": "(This post is from a talk on data leakage at the 2021 Technology and Measurement Around the Globe (TMAG) virtual symposium. Code and data can be found here)\n\n\nData leakage happens when information about the outcome variable “leaks” into the model training process, and such information is not available when the trained model is later used to make predictions on new data (Ambroise & McLachlan, 2002).\nFor example, data analysts may want use images (shown below) to predict whether a mole/melanoma is malignant or benign. Notice that one issue in these images is that a ruler is shown in the malignant image. If majority of the malignant images has a ruler, then the presence of a ruler will leak information about the outcome (malignant or benign) into the model training process: a ruler was shown because it was already diagnosed as malignant, and doctors probably wanted to track the size of the melanoma.\n\nWhen a model trained on such image data is used in real applications (, where there will be no rulers, because diagnosis has not been done), the model will become inaccurate. Therefore, data leakage may lead to bad model performance on new data and hinder model generalizability. In this example, one solution is to crop the images and only include information about the melanoma. Similarly, there are possibilities for data leakage in organizational research. Let’s look at some examples and discuss how to avoid this issue.\n\n\n\nWe will use an example dataset to demonstrate some common data leakage issues. Note that the dataset is simulated, it’s not real data.\nAssuming that we are interested in predicting employee turnover, and we have decided to train a model based on this dataset. Turnover is a binary variable (Y = yes, the employee has left; N = no, the employee hasn’t left). The predictors include the year the employee was hired (“year_hired”), age, gender, education, tenure in the organization (“tenure”), job satisfaction (“jobsat”), performance (“perf”), and salary. There are 200 observations in this dataset, the first ten rows are shown below:\nlibrary(caret)\n\nSEED <- 8424 # this random seed was for replicating the results\n\ndt <- read.csv(\"toydata.csv\")\nhead(dt, 10)\n##    year_hired age gender edu tenure jobsat perf salary turnover\n## 1        2012  36      2   3      4   3.86 1.93     NA        Y\n## 2        2017  NA      2   3      4   5.00   NA   5000        N\n## 3        2016  40      2   4      5   2.07   NA   6800        N\n## 4        2015  NA      1   3      6   3.34 2.84  10000        N\n## 5        2017  22      1   3      4     NA 5.00   5000        N\n## 6        2017  24      2   3      4   3.81 4.87   5000        N\n## 7        2015  NA      2   3      6   1.90 4.39  10000        N\n## 8        2002  57      2   3      3     NA 2.21     NA        Y\n## 9        2012  33      1   3      4   1.00   NA     NA        Y\n## 10       2019  25      2   2      2   3.17 3.77   5500        N\n\n\nThere are missing data in this dataset. For simplicity, let’s use mean imputation.\ndt$age[is.na(dt$age)] <- mean(dt$age, na.rm = TRUE)\ndt$jobsat[is.na(dt$jobsat)] <- mean(dt$jobsat, na.rm = TRUE)\ndt$perf[is.na(dt$perf)] <- mean(dt$perf, na.rm = TRUE)\ndt$salary[is.na(dt$salary)] <- mean(dt$salary, na.rm = TRUE)\n\nhead(dt, 10)\n##    year_hired      age gender edu tenure   jobsat     perf salary turnover\n## 1        2012 36.00000      2   3      4 3.860000 1.930000   6850        Y\n## 2        2017 35.96815      2   3      4 5.000000 3.389545   5000        N\n## 3        2016 40.00000      2   4      5 2.070000 3.389545   6800        N\n## 4        2015 35.96815      1   3      6 3.340000 2.840000  10000        N\n## 5        2017 22.00000      1   3      4 2.955568 5.000000   5000        N\n## 6        2017 24.00000      2   3      4 3.810000 4.870000   5000        N\n## 7        2015 35.96815      2   3      6 1.900000 4.390000  10000        N\n## 8        2002 57.00000      2   3      3 2.955568 2.210000   6850        Y\n## 9        2012 33.00000      1   3      4 1.000000 3.389545   6850        Y\n## 10       2019 25.00000      2   2      2 3.170000 3.770000   5500        N\n\n\n\nNow let’s set aside a test set for evaluating model performance on new data, and train the model on the remaining training data. We are using the elastic net algorithm and random parameter search via 10-fold cross-validation.\nset.seed(SEED)\n\n# train-test split, use a random 70% of the data for training\nidx <- sample(1:nrow(dt), floor(nrow(dt) * 0.70))\ndt_train <- dt[idx, ]\ndt_test <- dt[-idx, ]\n\n# specify the predictors\npredictors <- c(\"year_hired\", \"age\", \"gender\", \"edu\",\n                \"tenure\", \"jobsat\", \"perf\", \"salary\")\n\n# train the model\ncontrol <- trainControl(method = \"cv\",\n                        number = 10,\n                        search = \"random\",\n                        verboseIter = FALSE)\n\nfit1 <- train(x = dt_train[, predictors],\n                y = dt_train[, \"turnover\"],\n                metric = \"Accuracy\",\n                method = \"glmnet\",\n                trControl = control,\n                tuneLength = 30)\nLet’s check the prediction accuracy in the test set:\nyhat <- predict(fit1, dt_test[, predictors])\n\nsum(yhat == dt_test[, \"turnover\"]) / nrow(dt_test)\n## [1] 1\nIt looks like we achieved 100% accuracy. Such a result is almost impossible, and it probably suggests some data leakage issues.\n\n\n\n\nLet’s check the data again.\nhead(dt, 10)\n##    year_hired      age gender edu tenure   jobsat     perf salary turnover\n## 1        2012 36.00000      2   3      4 3.860000 1.930000   6850        Y\n## 2        2017 35.96815      2   3      4 5.000000 3.389545   5000        N\n## 3        2016 40.00000      2   4      5 2.070000 3.389545   6800        N\n## 4        2015 35.96815      1   3      6 3.340000 2.840000  10000        N\n## 5        2017 22.00000      1   3      4 2.955568 5.000000   5000        N\n## 6        2017 24.00000      2   3      4 3.810000 4.870000   5000        N\n## 7        2015 35.96815      2   3      6 1.900000 4.390000  10000        N\n## 8        2002 57.00000      2   3      3 2.955568 2.210000   6850        Y\n## 9        2012 33.00000      1   3      4 1.000000 3.389545   6850        Y\n## 10       2019 25.00000      2   2      2 3.170000 3.770000   5500        N\nNotice that in the data, the variable “salary” has an interesting pattern: salary is missing (imputed by the mean salary, $6,850) for employees who have left the organization. This makes sense because if an employee has left, we do not need to pay a salary anymore. But this directly implies that the employee has left, and the algorithm will be able to capture this relationship (salary = 6,850 -> turnover = “Y”, otherwise -> turnover = “N”). However, the goal of the model is to predict turnover, so we will not be able to observe such a pattern in “salary” when the model is used on future data. Therefore, we should not include this variable. If we want to use salary as a predictor, we should obtain salary information that does not indicate turnover, such as starting salary, annual salary, last paid salary, etc.\nLet’s remove the salary column and train the model again.\n# remove the salary variable\npredictors <- c(\"year_hired\", \"age\", \"gender\", \"edu\",\n                \"tenure\", \"jobsat\", \"perf\")\n\n# fit the model again\nset.seed(SEED)\nfit2 <- train(x = dt_train[, predictors],\n                y = dt_train[, \"turnover\"],\n                metric = \"Accuracy\",\n                method = \"glmnet\",\n                trControl = control,\n                tuneLength = 30)\n\n# check prediction results\nyhat <- predict(fit2, dt_test[, predictors])\nsum(yhat == dt_test[, \"turnover\"]) / nrow(dt_test)\n## [1] 1\nWe have removed the salary variable, but our model still achieved perfect accuracy. Let’s take a closer look.\n\n\n\nThe second issue requires the data analyst to have relevant knowledge about the data and task. In this example, “year_hired” and “tenure” together leaked information about turnover into model training.\nNotice that:\n\nFor employees who stayed in the organization, year_hired + tenure = current year\nFor employees who have left, year_hired + tenure < current year\n\nIn reality, this may be due to the data management workflow. For example, one database stores data about recruitment and hiring, and another database stores data regarding performance management. When the analyst merges data, he/she may include “year_hired” from the recruitment and hiring database, and “tenure” from the performance management database. To identify and avoid such kind of data leakage issues, the analyst should have specific knowledge about the problem.\nTo deal with this issue, let’s remove “year_hired”. After removing “year_hired”, the prediction accuracy is 80%.\nset.seed(SEED)\n\n# remove year_hired\npredictors <- c(\"age\", \"gender\", \"edu\",\n                \"tenure\", \"jobsat\", \"perf\")\n\n# fit the model again\nfit3 <- train(x = dt_train[, predictors],\n              y = dt_train[, \"turnover\"],\n              metric = \"Accuracy\",\n              method = \"glmnet\",\n              trControl = control,\n              tuneLength = 30)\n\n# check prediction results\nyhat <- predict(fit3, dt_test[, predictors])\nsum(yhat == dt_test[, \"turnover\"]) / nrow(dt_test)\n## [1] 0.8\n\n\n\nSo far we have looked at two examples of column-wise leakage, or leaking features. There is also another form of leakage called row-wise leakage, or leakage in training data (Kaufman et al., 2012).\nRecall that we imputed missing data using the means of the whole sample (which contained both thr training and test set). But this is not possible because we in reality we only have the training set. To prevent data leakage, we have to pretend that we don’t have access to the test set. Imputing missing data using the whole sample will expose the model to some information about the test data.\nTherefore, missing data imputation should only happen within the training set. Operationally, we should do train-test split first, then impute missing data (also see a note at the end).\nLet’s fix this issue:\n# reload data\ndt <- read.csv(\"toydata.csv\")\n\n# train-test split first\ndt_train <- dt[idx, ]\ndt_test <- dt[-idx, ]\n\n# impute missing data in the training set\ndt_train$age[is.na(dt_train$age)] <- mean(dt_train$age, na.rm = TRUE)\ndt_train$jobsat[is.na(dt_train$jobsat)] <- mean(dt_train$jobsat, na.rm = TRUE)\ndt_train$perf[is.na(dt_train$perf)] <- mean(dt_train$perf, na.rm = TRUE)\n\n# do the same to test set\ndt_test$age[is.na(dt_test$age)] <- mean(dt_train$age, na.rm = TRUE)\ndt_test$jobsat[is.na(dt_test$jobsat)] <- mean(dt_train$jobsat, na.rm = TRUE)\ndt_test$perf[is.na(dt_test$perf)] <- mean(dt_train$perf, na.rm = TRUE)\n# notice that we used means of the training set to impute missing data in the test set\n\n# fit the model again\nset.seed(SEED)\n\nfit4 <- train(x = dt_train[, predictors],\n              y = dt_train[, \"turnover\"],\n              metric = \"Accuracy\",\n              method = \"glmnet\",\n              trControl = control,\n              tuneLength = 30)\n\n# check prediction results\nyhat <- predict(fit4, dt_test[, predictors])\nsum(yhat == dt_test[, \"turnover\"]) / nrow(dt_test)\n## [1] 0.7\n\n\n\nFrom the above example, we can see how data leakage can cause the model to perform unrealistically well in the available data. Such good model performance is almost certainly not going to happen in real applications. Thus, detecting and avoiding data leakage is crucial.\nFirst of all, while a prediction accuracy of 100% often means leakage (especially column-wise leakage, as in two of our examples), we cannot rely on a 100% prediction accuracy to detect data leakage. In our example, “year_hired” and “tenure” together determined turnover, but in real-world datasets, there might be missing data issues or errors when inputting data, making prediction accuracy not perfectly 100%, even when data leakage is present.\nThere are two steps we can take to avoid column-wise leakage:\n\nCarefully select predictors, avoid predictors that imply the outcome. Exploratory data analysis often helps. For example, if a variable is highly correlated (e.g., correlation is close to 1) with the outcome, then a more careful inspection of the nature of the variable is probably needed.\nAcquire domain-specific knowledge about the problem. This is highly context-dependent and there is not a standard solution. In our example, the analyst should be sensitive of potential relationships in the data stored in different databases, and work closely with coworkers who are in charge of collecting and managing employee data. Also, the analyst should have relevant knowledge about organizational tenure to avoid issues such as “year_hired” + “tenure” => “turnover”.\n\nTo avoid row-wise leakage, a healthy model training pipeline is very beneficial. In our example, we should do train-test split first, and set the test data aside. After this, we preprocess training data, use training data to process the test data, and never use any information from the test data. A pipeline can get very complicated. Popular machine learning packages (e.g., “tidymodels” and “caret” in R, “scikit-learn” in Python) have tools to build up pipelines.\nNote. The data preprocessing procedure (i.e., train-test split first, then impute missing data in the training set only) was in fact not ideal and for demonstration purposes only. One should incorporate data preprocessing into the cross-validation process, and it can be treated as a part of the model parameter as well. For example, a model training pipeline can be established so that it searches the best missing data imputation strategy based on cross-validation."
  },
  {
    "objectID": "posts/2021-12-04-hierarchical-model/index.html",
    "href": "posts/2021-12-04-hierarchical-model/index.html",
    "title": "Hierarchical Models",
    "section": "",
    "text": "\\[\\begin{aligned}\ny_{ij} | \\mu_j, \\sigma^2 &\\overset{i.i.d.}\\sim \\mathcal{N}(\\mu_j, \\sigma^2),\n\\end{aligned}\\] where \\(i = 1, ..., n_j\\), (number of observations in group \\(j\\)), and \\(j = 1, ..., J\\), (number of groups)."
  },
  {
    "objectID": "posts/2021-12-04-hierarchical-model/index.html#priors",
    "href": "posts/2021-12-04-hierarchical-model/index.html#priors",
    "title": "Hierarchical Models",
    "section": "Priors",
    "text": "Priors\n\\[\\begin{aligned}\n\\mu_j | \\mu, \\tau^2 &\\sim \\mathcal{N}(\\mu, \\tau^2) \\\\\n\\mu | \\mu_0, \\tau_0^2 &\\sim \\mathcal{N}(\\mu_0, \\tau_0^2) \\\\\n\\tau^2 | \\alpha_\\tau, \\beta_\\tau &\\sim \\mathcal{IG}(\\alpha_\\tau, \\beta_\\tau) \\Rightarrow 1/\\tau^2 | \\alpha_\\tau, \\beta_\\tau \\sim \\mathcal{G}(\\alpha_\\tau, \\beta_\\tau) \\\\\n\\sigma^2 | \\alpha_\\sigma, \\beta_\\sigma &\\sim \\mathcal{IG}(\\alpha_\\sigma, \\beta_\\sigma) \\Rightarrow 1/\\sigma^2 | \\alpha_\\sigma, \\beta_\\sigma \\sim \\mathcal{G}(\\alpha_\\sigma, \\beta_\\sigma)\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/2021-12-04-hierarchical-model/index.html#likelihood",
    "href": "posts/2021-12-04-hierarchical-model/index.html#likelihood",
    "title": "Hierarchical Models",
    "section": "Likelihood",
    "text": "Likelihood\n\\[\\begin{aligned}\nL(y_{ij} | \\mu_j, \\mu, \\sigma^2, \\tau^2) &= \\prod_{j=1}^J \\Big\\{\\prod_{i = 1}^{n_j}\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}exp\\Big[-\\frac{1}{2}\\frac{(y_{ij} - \\mu_j)^2}{\\sigma^2}\\Big]\\Big\\}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/2021-12-04-hierarchical-model/index.html#posterior",
    "href": "posts/2021-12-04-hierarchical-model/index.html#posterior",
    "title": "Hierarchical Models",
    "section": "Posterior",
    "text": "Posterior\n\\[\\begin{aligned}\np(&\\mu_j, \\mu, \\sigma^2, \\tau^2 | y_{ij}, \\mu_0, \\tau_0^2, \\alpha_\\tau, \\beta_\\tau, \\alpha_\\sigma, \\beta_\\sigma) \\propto \\text{ Likelihood} \\times \\text{Prior}  \\\\\n\\propto &L(y_{ij} | \\mu_j, \\mu, \\sigma^2, \\tau^2) \\prod_{j=1}^J \\Big[p(\\mu_j|\\mu, \\tau^2)\\Big]p(\\mu|\\mu_0, \\tau_0^2)p(1/\\tau^2|\\alpha_\\tau, \\beta_\\tau)p(1/\\sigma^2|\\alpha_\\sigma, \\beta_\\sigma) \\\\\n=&\\prod_{j=1}^J \\Big\\{\\prod_{i = 1}^{n_j}\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}exp\\Big[-\\frac{1}{2}\\frac{(y_{ij} - \\mu_j)^2}{\\sigma^2}\\Big]\\Big\\} \\\\\n&\\prod_{j=1}^J\\Big[\\frac{1}{\\sqrt{2 \\pi \\tau^2}}exp\\Big[-\\frac{1}{2}\\frac{(\\mu_j - \\mu)^2}{\\tau^2}\\Big] \\\\\n&\\Big[\\frac{1}{\\sqrt{2 \\pi \\tau_0^2}}exp\\Big[-\\frac{1}{2}\\frac{(\\mu - \\mu_0)^2}{\\tau_0^2}\\Big] \\\\\n& \\frac{\\beta_\\tau^{\\alpha_\\tau}}{\\Gamma(\\alpha_\\tau)}(1/\\tau^2)^{\\alpha_\\tau-1}exp[-\\beta_\\tau(1/\\tau^2)] \\\\\n& \\frac{\\beta_\\sigma^{\\alpha_\\sigma}}{\\Gamma(\\alpha_\\sigma)}(1/\\sigma^2)^{\\alpha_\\sigma-1}exp[-\\beta_\\sigma(1/\\sigma^2)]\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/2021-12-04-hierarchical-model/index.html#full-conditionals",
    "href": "posts/2021-12-04-hierarchical-model/index.html#full-conditionals",
    "title": "Hierarchical Models",
    "section": "Full conditionals",
    "text": "Full conditionals\n\\[\\begin{aligned}\n\\mu_j | . &\\sim \\mathcal{N}\\Big(\\frac{\\sum_{i=1}^{n_j} y_{ij} / \\sigma^2 + \\mu / \\tau^2}{n_j / \\sigma^2 + 1/\\tau^2}, \\frac{1}{n_j/\\sigma^2 + 1/\\tau^2}\\Big) \\\\\n\\mu |. &\\sim \\mathcal{N}\\Big(\\frac{\\sum_{j=1}^J \\mu_j/\\tau^2 + \\mu_0/\\tau_0^2}{J/\\tau^2 + 1/\\tau_0^2}, \\frac{1}{J/\\tau^2 + 1/\\tau_0^2}\\Big) \\\\\n1/\\tau^2 |. &\\sim \\mathcal{G}\\Big(\\alpha_\\tau + \\frac{J}{2}, \\beta_\\tau + \\frac{\\sum_{j=1}^J (\\mu_j - \\mu)^2}{2}\\Big) \\\\\n1/\\sigma^2 |. &\\sim \\mathcal{G}\\Big(\\alpha_\\sigma + \\frac{\\sum_{j=1}^J n_j}{2}, \\beta_\\sigma + \\frac{\\sum_{j=1}^J \\sum_{i=1}^{n_j}(y_{ij}-\\mu_j)^2}{2}\\Big)\n\\end{aligned}\\]\n(image credit: https://stats.stackexchange.com/q/44583)"
  },
  {
    "objectID": "posts/2022-07-02-correlation-vs-causation/index.html",
    "href": "posts/2022-07-02-correlation-vs-causation/index.html",
    "title": "Correlation vs. Causation",
    "section": "",
    "text": "A great way to understand correlation \\(\\ne\\) causation :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chen Tang",
    "section": "",
    "text": "Research Methods\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2022\n\n\nChen Tang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayesian\n\n\nStatistics\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2021\n\n\nChen Tang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMath\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2021\n\n\nChen Tang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMath\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2020\n\n\nChen Tang\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine learning\n\n\nBias-variance trade-off\n\n\nPrediction\n\n\nMath\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2019\n\n\nChen Tang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Chen Tang",
    "section": "",
    "text": "Chen Tang\n(Updated: June 2022)\nUniversity of Illinois Urbana-Champaign\n504 E Armory Ave, Champaign, IL 61820\ntangc@tangc.net"
  },
  {
    "objectID": "cv.html#research-interests",
    "href": "cv.html#research-interests",
    "title": "Chen Tang",
    "section": "RESEARCH INTERESTS",
    "text": "RESEARCH INTERESTS\nDiversity in HR Management/Personnel Selection (Adverse impact, equal and fair access to employment)\nHR Analytics and Machine Learning (Multi-objective optimization, ensemble learning, cross-validation, Bayesian statistics)\nBehavioral Ethics (Unethical Pro-organizational behavior)"
  },
  {
    "objectID": "cv.html#honors-and-awards",
    "href": "cv.html#honors-and-awards",
    "title": "Chen Tang",
    "section": "HONORS AND AWARDS",
    "text": "HONORS AND AWARDS\n2022: Meredith P. Crawford Fellowship, Human Resources Research Organization (HumRRO)\n2022: International Personnel Assessment Council James C. Johnson Student Paper Award\n2022: Timothy A. Judge Fellowship, School of Labor and Employment Relations, UIUC\n2020: University of Illinois List of Teachers Ranked as Excellent (Fall 2020)"
  },
  {
    "objectID": "cv.html#academic-experience",
    "href": "cv.html#academic-experience",
    "title": "Chen Tang",
    "section": "ACADEMIC EXPERIENCE",
    "text": "ACADEMIC EXPERIENCE\nResearch Assistant, School of Labor and Employment Relations, University of Illinois Urbana-Champaign, 2017-Present\nResearch Assistant, School of Entrepreneurship and Management, ShanghaiTech University, 2015-2017"
  },
  {
    "objectID": "cv.html#teaching-experience",
    "href": "cv.html#teaching-experience",
    "title": "Chen Tang",
    "section": "TEACHING EXPERIENCE",
    "text": "TEACHING EXPERIENCE\nTeaching Assistant, LER 590 DDD (HR Analytics), School of Labor and Employment Relations, UIUC, Fall 2022\nInstructor, PSYC 245 (Introduction to Industrial-Organizational Psychology), Department of Psychology, UIUC, Spring 2022\nTeaching Assistant, LER 593 (Statistics for graduate students in human resources), School of Labor and Employment Relations, UIUC, Fall 2020\nTeaching Assistant, Negotiation, School of Entrepreneurship and Management, ShanghaiTech University, Spring 2016 and Spring 2017\nTeaching Assistant, Executive MBA Midterm Module, China Europe International Business School, Spring 2017"
  },
  {
    "objectID": "cv.html#refereed-journal-publications",
    "href": "cv.html#refereed-journal-publications",
    "title": "Chen Tang",
    "section": "REFEREED JOURNAL PUBLICATIONS",
    "text": "REFEREED JOURNAL PUBLICATIONS\nSong, Q. C., Shin, H. J., Tang, C., Hanna, A., & Behrend, T. S. (2022). Investigating machine learning’s capacity to enhance the prediction of career choices. Personnel Psychology, 1-25. https://doi.org/10.1111/peps.12529\nNewman, D. A., Tang, C., Song, Q. C., & Wee, S. (accepted). Dropping the GRE, keeping the GRE, or using GRE-optional admissions? Considering test fairness and Pareto-optimal tradeoffs. International Journal of Testing. https://doi.org/10.1080/15305058.2021.2019750\nSong, Q. C., Tang, C., & Wee, S. (2021). Making sense of model generalizability: A tutorial on cross-validation in R and Shiny. Advances in Methods and Practices in Psychological Science. 4(1): 1-17. https://doi.org/10.1177/2515245920947067\nYang, Y., Tang, C., Qu, X., Wang, C., & Denson, T. F. (2018). Group facial width-to-height ratio predicts intergroup negotiation outcomes. Frontiers in Psychology, 9. https://doi.org/10.3389/fpsyg.2018.00214"
  },
  {
    "objectID": "cv.html#manuscripts-under-review",
    "href": "cv.html#manuscripts-under-review",
    "title": "Chen Tang",
    "section": "MANUSCRIPTS UNDER REVIEW",
    "text": "MANUSCRIPTS UNDER REVIEW\nTang, C., Hickman, L., Song, Q. C., & Alexander III, L. (1st R&R). [Details omitted for blind reviewing]. Journal of Applied Psychology.\nSong, Q. C., Tang, C., Newman, D. A., & Wee, S. (2nd R&R). [Details omitted for blind reviewing]. Journal of Applied Psychology.\nSong, Q. C., Tang, C., & Wee, S. (1st R&R). [Details omitted for blind reviewing]. Journal of Applied Psychology.\nTang, C., Chen, Y., Wei, W, & Newman, D. A. (2nd R&R, revision submitted). [Details omitted for blind reviewing]. Journal of Business Ethics.\nSong, Q. C., Tang, C., Alexander III, L., Hickman, L., & Kim, Y. (2nd R&R). [Details omitted for blind reviewing]. Personnel Psychology."
  },
  {
    "objectID": "cv.html#manuscripts-in-preparation",
    "href": "cv.html#manuscripts-in-preparation",
    "title": "Chen Tang",
    "section": "MANUSCRIPTS IN PREPARATION",
    "text": "MANUSCRIPTS IN PREPARATION\nTang, C., Newman, D. A., Song, Q. C., & Wee, S. (to be submitted). Shrinkage of diversity tradeoff curves in personnel selection: A comparison of local validity studies, meta-analysis, Bayes-analysis, and ensemble machine learning.\nTang, C., Zhang. B., Lin. Z., Anglim. J., & Li. J. (to be submitted). Faking detection using item-level machine learning.\nTang, C., Shin, H. J., & Song, Q. C. (writing in progress). Combining machine learning models to improve prediction accuracy: The use of ensemble learning in organizational research and practice.\nTang, C., Song, Q. C., & Hickman, L. (writing in progress). Detecting and avoiding common data leakage issues when using machine learning in organizational research.\nCulpepper, S. A. & Tang, C. (model implementation). Rank likelihood for non-normal data in Bayesian multilevel models.\nTang, C., Chen, Y., Song, Q. C., & Newman, D. A. (writing in progress). Valuing diversity in hiring when choosing a predictor weighting method: An extension of Sackett et al. (2017).\n*Lee, S. H., *Tang, C., Wei, W., & Chen, Y. (writing in progress). [Details omitted for blind reviewing]. (* Equal contribution)\nLee, SH., & Tang, C. (data collection). Won’t stop searching: Detachment, self-improvement, and job search outcomes."
  },
  {
    "objectID": "cv.html#book-chapterssections",
    "href": "cv.html#book-chapterssections",
    "title": "Chen Tang",
    "section": "BOOK CHAPTERS/SECTIONS",
    "text": "BOOK CHAPTERS/SECTIONS\nTang, C., Liu, Y. (forthcoming). Agent-based modeling. In Vancouver, J. B., Wang, M., & Weinhardt, J. M. (eds.), SIOP Frontiers Series: Computational Modeling for Industrial-Organizational Psychologists. Washington, DC: American Psychological Association.\nSong, Q. C., Liu, M. Q., Tang, C., & Long, L. (2020). Applying principles of big data to the workplace and talent analytics. Big Data Methods for Psychological Research: New Horizons and Challenges. Washington D. C., APA books.\nTang, C., & Yang, Y. (2017). Goals. In Zeigler-Hill, V., & Shackelford, T. K. (Eds.) Encyclopedia of Personality and Individual Difference. Springer."
  },
  {
    "objectID": "cv.html#conference-presentations",
    "href": "cv.html#conference-presentations",
    "title": "Chen Tang",
    "section": "CONFERENCE PRESENTATIONS",
    "text": "CONFERENCE PRESENTATIONS\nTang, C., Newman, D. A., Song, Q. C. & Wee, S. (July 2022). Shrinkage of diversity tradeoff curves in personnel selection: A comparison of local validity studies, meta-analysis, Bayes-analysis, and ensemble machine learning [Invited presentation; Winner of the James C. Johnson Student Paper Award]. The 2022 Annual Conference of the International Personnel Assessment Council, San Diego, CA.\nTang, C., Newman, D. A., Song, Q. C. & Wee, S. (April 2022). Shrinkage of diversity tradeoff curves in personnel selection: A comparison of local validity studies, meta-analysis, and empirical Bayes-analysis. In Song, Q. C., & Wee, S. (Co-Chairs), Multi-Objective Optimization in the Workplace 3.0: Advancing research on adverse impact in personnel selection [Symposium paper]. 37th Annual Conference of the Society for Industrial and Organizational Psychology, Seattle, WA.\nSong, Q. C., Shin, H. J., Tang, C., Hanna, A. & Behrend, T. S., (April 2022). Machine learning enhances the prediction of career choices. In Chris Nye (Chair), The power of vocational interests: Understanding choices, attitudes, and behavior [Symposium paper]. 37th Annual Conference of the Society for Industrial and Organizational Psychology, Seattle, WA.\nTang, C. (June 2021). Common data leakage issues when using machine learning in organizational research [Talk]. Technology and Measurement Around the Globe. (Purdue University, West Lafayette, IN. Online Conference)\nLee, SH., Tang, C., Wei, W., & Chen, Y. (August 2021). The Roles of Compassion and Resilience on Employee Thriving, Well-being, and Interpersonal Deviance [Paper]. 81st Annual Meeting of the Academy of Management. (Online Conference)\nTang, C., Hickman, L., Song, Q. C., & Alexander III, L. (April 2021). Comparing Item-Level and Scale-Level Predictive Models: A Simulation [Poster]. 36th Annual Convention of the Society for Industrial and Organizational Psychology, New Orleans, LA. (Online Conference)\nTang, C., Newman, D. A., Song, Q. C., & Wee., S. (April 2021). Pareto-Optimal Tradeoffs for Three Notions of Test Fairness. In Song, Q. C. (Chair), Multi-Objective Optimization in the Workplace 2.0: Applications in Selection [Symposium paper]. 36th Annual Convention of the Society for Industrial and Organizational Psychology, New Orleans, LA. (Online Conference)\nTang, C., Chen, Y., Wei, W. (August 2020). External work locus of control and unethical pro-organizational behavior: A dual-path model [Paper]. 80th Annual Meeting of the Academy of Management, Vancouver, British Columbia, Canada. (Online Conference)\nTang, C., Newman, D. A., & Song, Q. C. (April 2020). Addressing diversity-validity tradeoffs via Pareto weights with orthogonalized criteria. In Song, Q. C. & Wee. S. (Co-chairs), Multi-Objective Optimization in the Workplace: Addressing Adverse Impact in Selection [Symposium paper]. 35th Annual Convention of the Society for Industrial and Organizational Psychology, Austin, TX. (Online Conference)\nSong, Q. C., & Tang, C. (April 2020). Adverse impact reduction for multiple subgroups: A Pareto-optimization approach. In Song, Q. C. & Wee. S. (Co-chairs), Multi-Objective Optimization in the Workplace: Addressing Adverse Impact in Selection [Symposium paper]. 35th Annual Convention of the Society for Industrial and Organizational Psychology, Austin, TX. (Online Conference)\nTang, C., Shin, H. J., Barve, A., & Song, Q., C. (April 2020). Using ensemble machine learning to improve assessment in personnel selection [Poster]. 35th Annual Convention of the Society for Industrial and Organizational Psychology, Austin, TX. (Online Conference)\nLee, S. H., Tang, C., & Liang, Y. J. (April 2020). Won’t stop searching: Detachment, self-improvement, and job search outcomes [Poster]. 35th Annual Convention of the Society for Industrial and Organizational Psychology, Austin, TX. (Online Conference)\nTang, C., Chen, Y., Song, Q. C., & Newman, D. A. (April 2019). Predictor weighting with adverse impact and shrinkage: Reply to Sackett et al. (2017) [Poster]. 34th Annual Convention of the Society for Industrial and Organizational Psychology, Washington, DC."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Chen Tang",
    "section": "EDUCATION",
    "text": "EDUCATION\n\nSchool of Labor and Employment Relations, University of Illinois Urbana-Champaign\nPh.D. in Human Resources and Industrial Relations, May 2023 (expected)\n\n\nSchool of Psychology and Cognitive Science, East China Normal University\nM.Ed. in Applied Psychology, June 2012 B.S. in Psychology, June 2009"
  }
]